{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DivNoising - Training\n",
    "This notebook contains an example on how to train a DivNoising VAE.  This requires having a noise model (model of the imaging noise) which can be either measured from calibration data or estimated from raw noisy images themselves. If you haven't done so, please first run either ```0a-CreateNoiseModel.ipynb``` (if you have calibration data to create a noise model from) or the ntebook ```0b-CreateNoiseModel.ipynb``` if you do not have calibration data and you wish to use noisy images themselves to bootstrap a noise model. These notebooks will download the data and create a noise model. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T07:07:22.605056Z",
     "start_time": "2025-12-15T07:07:18.695430Z"
    }
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We import all our dependencies.\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from tifffile import imread\n",
    "from divnoising import utils, training\n",
    "from divnoising.gaussianMixtureNoiseModel import GaussianMixtureNoiseModel\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "if not torch.cuda.is_available():\n",
    "    raise ValueError(\"GPU not found, code will run on CPU and can be extremely slow!\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify ```path``` to load data\n",
    "Your data should be stored in the directory indicated by ```path```. This notebook expects 2D datasets in ```.tif``` format. If your data is a stack of 2D images, you can load it as shown in the next cell. If you dataset has multiple individual 2D tif files, comment out the second line in the cell below and uncomment the third line."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T07:23:45.444644Z",
     "start_time": "2025-12-15T07:23:19.116417Z"
    }
   },
   "source": [
    "observation= imread('../../data/SDTrans/noise_200Hz_2400frames_pxlsize30nm_3.90dBSNR_24000x328x328.tif')\n",
    "observation = observation[:1000]"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build the spatial encoding"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T07:24:03.158360Z",
     "start_time": "2025-12-15T07:24:00.811991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "H, W = observation.shape[1], observation.shape[2]\n",
    "\n",
    "# buile the x, y position\n",
    "x = np.linspace(0, 1, W)\n",
    "y = np.linspace(0, 1, H)\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "pos = np.stack([xv, yv], axis=0)  # shape: [2, H, W]\n",
    "pos = np.repeat(pos[np.newaxis], observation.shape[0], axis=0)  # shape: [1000, 2, H, W]\n",
    "\n",
    "observation = np.expand_dims(observation, axis=1)\n",
    "\n",
    "observation = np.concatenate([observation, pos], axis=1)  # shape -> [1000, 3, 328, 328]\n",
    "print(observation.shape)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3, 328, 328)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we need to follow some preprocessing steps first which will prepare the data for training purposes. <br>\n",
    "We first divide the data randomly into training and validation sets with 85% images allocated to training set  and rest to validation set. <br>\n",
    "Then we augment the training data 8-fold by 90 degree rotations and flips, set `augment` flag to `False` to disable data augmentation. <br>\n",
    "After that patches of size `patch_size` are extracted from training and validation images. <br>\n",
    "Finally, we compute the mean and standard deviation of our combined train and validation sets and do some additional preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T07:24:08.114946Z",
     "start_time": "2025-12-15T07:24:07.288020Z"
    }
   },
   "source": [
    "train_patches, val_patches = utils.get_trainval_patches(observation,augment=True,patch_size=328)\n",
    "x_train_tensor, x_val_tensor, data_mean, data_std = utils.preprocess(train_patches, val_patches)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/850 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'max_patches' parameter of extract_patches_2d must be a float in the range (0.0, 1.0), an int in the range [1, inf) or None. Got 0 instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mInvalidParameterError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m train_patches, val_patches = \u001B[43mutils\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_trainval_patches\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\u001B[43maugment\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43mpatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m328\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m x_train_tensor, x_val_tensor, data_mean, data_std = utils.preprocess(train_patches, val_patches)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/autodl-tmp/divnoising/utils.py:89\u001B[39m, in \u001B[36mget_trainval_patches\u001B[39m\u001B[34m(x, split_fraction, augment, patch_size, num_patches)\u001B[39m\n\u001B[32m     87\u001B[39m train_images = x[:\u001B[38;5;28mint\u001B[39m(split_fraction*x.shape[\u001B[32m0\u001B[39m])]\n\u001B[32m     88\u001B[39m val_images = x[\u001B[38;5;28mint\u001B[39m(split_fraction*x.shape[\u001B[32m0\u001B[39m]):]\n\u001B[32m---> \u001B[39m\u001B[32m89\u001B[39m x_train_crops = \u001B[43mextract_patches\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_patches\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     90\u001B[39m x_val_crops = extract_patches(val_images, patch_size, num_patches)\n\u001B[32m     91\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m(augment):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/autodl-tmp/divnoising/utils.py:114\u001B[39m, in \u001B[36mextract_patches\u001B[39m\u001B[34m(x, patch_size, num_patches)\u001B[39m\n\u001B[32m    111\u001B[39m patches = np.zeros(shape=(x.shape[\u001B[32m0\u001B[39m]*num_patches,patch_size,patch_size))\n\u001B[32m    113\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(x.shape[\u001B[32m0\u001B[39m])):\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m     patches[i*num_patches:(i+\u001B[32m1\u001B[39m)*num_patches] = \u001B[43mimage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mextract_patches_2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43mpatch_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_patches\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_patches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[43m                                                                       \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m=\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m    \n\u001B[32m    116\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m patches\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:208\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    205\u001B[39m to_ignore += [\u001B[33m\"\u001B[39m\u001B[33mself\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcls\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    206\u001B[39m params = {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m params.arguments.items() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m to_ignore}\n\u001B[32m--> \u001B[39m\u001B[32m208\u001B[39m \u001B[43mvalidate_parameter_constraints\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    209\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparameter_constraints\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaller_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__qualname__\u001B[39;49m\n\u001B[32m    210\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    212\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    213\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m    214\u001B[39m         skip_parameter_validation=(\n\u001B[32m    215\u001B[39m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m    216\u001B[39m         )\n\u001B[32m    217\u001B[39m     ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:98\u001B[39m, in \u001B[36mvalidate_parameter_constraints\u001B[39m\u001B[34m(parameter_constraints, params, caller_name)\u001B[39m\n\u001B[32m     92\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     93\u001B[39m     constraints_str = (\n\u001B[32m     94\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join([\u001B[38;5;28mstr\u001B[39m(c)\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mconstraints[:-\u001B[32m1\u001B[39m]])\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m or\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     95\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconstraints[-\u001B[32m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     96\u001B[39m     )\n\u001B[32m---> \u001B[39m\u001B[32m98\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m InvalidParameterError(\n\u001B[32m     99\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mThe \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam_name\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[33m parameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcaller_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must be\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    100\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconstraints_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam_val\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[33m instead.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    101\u001B[39m )\n",
      "\u001B[31mInvalidParameterError\u001B[39m: The 'max_patches' parameter of extract_patches_2d must be a float in the range (0.0, 1.0), an int in the range [1, inf) or None. Got 0 instead."
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure DivNoising model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specify some parameters of our DivNoising network needed for training. The default parameters work well for most datasets.<br> \n",
    "\n",
    "$(i)$ The parameter <code>n_depth</code> specifies the depth of the network. <br> \n",
    "$(ii)$ <code>batch_size</code> specifies the batch size used for training. <br>\n",
    "$(iii)$ <code>max_epochs</code> specifies the maximum number of training epochs. In practice, the training may termionate earlier if the validation loss does not improve for $30$ epochs. This is called [early stopping](https://keras.io/api/callbacks/early_stopping/). Currently, we have set ```max_epochs``` heuristically to compute it depending on the number of training patches such that 22 million steps will be taken in total with the entire data seen in each epoch. <br>\n",
    "$(iv)$<code>model_name</code> specifies the name of the model with which the weights will be saved for prediction later. <br>\n",
    "$(v)$<code>basedir</code> is the directory where the model and some logs will be saved during training. If this directory does not exist already, it will be created automatically. <br>\n",
    "$(vi)$ Set <code>real_noise</code> to ```True``` if your dataset is intrinsically noisy. If your dataset is however corrupted synthetically by Gaussian noise, set this parameter to ```False```. If you choose ```real_noise=True```, then you will need to specify a noise model which you should have obtained by running either the notebook ```0a-CreateNoiseModel.ipynb``` or ```0b-CreateNoiseModel.ipynb```. If you have not yet generated the noise model for this dataset yet run that notebook first. \n",
    "If you choose ```real_noise=False```, then you should specify the standard deviation of the Gaussian noise used for synethtically corrupting the data. <br>\n",
    "$(vii)$<code>noise_model</code> is the noise model you want to use. For real microscopy datasets which are intrinsically noisy, first run the notebook  ```0-CreateNoiseModel.ipynb```, if you have not yet generated the noise model for this dataset yet. <br>\n",
    "$(viii)$<code>gaussian_noise_std</code> is only applicable if the dataset was synthetically corrupted with Gaussian noise. In this case, this parameter should be set to the standard deviation of the Gaussian noise used for synthetically corrupting the data. <br>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T07:08:45.369408Z",
     "start_time": "2025-12-15T07:08:45.202514Z"
    }
   },
   "source": [
    "n_depth=2\n",
    "batch_size=32\n",
    "max_epochs=int(22000000/(x_train_tensor.shape[0]))\n",
    "model_name = 'divnoising_SDTrans_demo' # a name used to identify the model\n",
    "basedir = 'models' # the base directory in which our model will live\n",
    "real_noise=True\n",
    "\n",
    "if real_noise:\n",
    "    noise_model_params= np.load(\"GMMNoiseModel_SDTrans_3_2_bootstrap.npz\")\n",
    "    noise_model = GaussianMixtureNoiseModel(params = noise_model_params, device = device)\n",
    "    gaussian_noise_std = None\n",
    "else:\n",
    "    gaussian_noise_std = 25 # set it to the correct value if using data synthetically corrupted by Gaussian noise."
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Tensorboard\n",
    "Running the cell below will start a tensorboard instance within this notebook where we can monitor training progress after it starts. If you additionally wish to see tensorboard in a separate browser tab, run the cell first and go to the link http://localhost:6006/  with your browser. The advantage of viewing Tensorboard in a separate tab is that more screen space is allocated to Tensorboard, hence leading to a nicer visualization."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T07:08:51.399082Z",
     "start_time": "2025-12-15T07:08:49.854960Z"
    }
   },
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./lightning_logs"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c5593760df8d7a49\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c5593760df8d7a49\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "84e63db3602597a43fd3c998b3ae0078"
     }
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network\n",
    "Running the cell below trains the network and saves the model weights into the directory specified by ```basedir``` earlier. The best and the last models are saved along with tensorboard logs.\n",
    "\n",
    "The number of network parameters and GPU/CPU usage information can be printed by setting the ```log_info``` parameter to ```True```. By default, it is turned off.\n",
    "\n",
    "__Note:__ We observed that for certain datasets, it may happen that the training is getting aborted multiple times with the message `posterior collapse: aborting`. \n",
    "This happens when the KL loss goes towards 0. This phenomenon is called ```posterior collapse``` and is undesirable.\n",
    "We try to prevent it by automatically checking the KL loss and aborting the training and restarting another training run once the KL drops below a threshold (```1e-7```). <br>\n",
    "\n",
    "But if this happens for $20$ times, we start a procedure called [kl annealing](https://arxiv.org/abs/1511.06349) which is a way to avoid ```posterior collapse```. In this case, we increase the weight on KL divergence loss term from 0 to 1 gradually in a number of epochs (the default number of epochs for kl annealing is set to $10$ epochs). With kl annealing, we attempt the training again."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "training.train_network(x_train_tensor, x_val_tensor, batch_size, data_mean, data_std, \n",
    "                       gaussian_noise_std, noise_model, n_depth=n_depth, max_epochs=max_epochs, \n",
    "                       model_name=model_name, basedir=basedir, log_info=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
